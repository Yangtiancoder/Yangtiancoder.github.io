---
layout:     post
title: 无监督学习中的聚类算法
category: blog
description: 毕设初涉。
---


由于最近开始搞毕设的东西，毕设也是与机器学习 有关的内容大致是基于亲和力传播的交通道路划分，一个虽然听不懂但是听起来感觉简单的一个题目，因此开始入了聚类算法的坑，查了一些手边的资料整理了一下。

 首先要区分开监督学习和无监督学习，例如我之前的机器学习笔记系列的模型啊之类的都是有监督学习，至于无监督学习常见的例如聚类，如何作为二者的区分呢，很明显，监督顾名思义将有答案的数据集扔进去学习，学习是知道了样本的答案可以进行优化学习有答案作为参照起到了一种“监督”的效果，那么无监督学习及为只有样本（特征），没有答案，二者可以抽象为答题中的主观题和客观题，有监督是客观题，无监督是主观题，以上为个人理解如有不妥之处请多多指教，用规范化的语言来说就看输入数据是否有标签（label）。输入数据有标签，则为有监督学习，没标签则为无监督学习。

对具有标签（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。因此，训练样本的岐义性低。  对没有标签（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。这里，所有的标记（分类）是未知的。因此，训练样本的岐义性高。

那么我们就要问了，既然无监督学习歧义性高，为什么还要使用他呢，存在必然是有道理的，在某些情况下我们不得不适用无监督学习，这不代表他不好，只是二者应用场景不同，比如当有监督学习样本获取代价很高的时候，我们很难找出大量带有标签的特征，没有充分的数据集，监督学习未必能取得较好的效果。



什么是聚类：



将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类。由聚类所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，与其他簇中的对象相异。              ——来自百度百科



聚类和分类的区别：



刚开始一直以为聚类和分类是一种东西，也把近邻传播算法和K近邻法搞混了，其实最根本的区别就是监督学习和无监督学习的区别，其次聚类需要解决的问题是将已给定的若干无标记的模式聚集起来使之成为有意义的聚类，聚类是在预先不知道目标数据库到底有多少类的情况下，希望将所有的记录组成不同的类或者说聚类，并且使得在这种分类情况下，以某种度量（例如：距离）为标准的相似性，在同一聚类之间最小化，而在不同聚类之间最大化



聚类原理：



1、先随机产生（选取数据集中的地方，收敛会比较快）每个类别的中心点（设定多少类别就产生多少个类的中心点）；
2、计算每个样本和中心点之间的距离，离哪个最近，就将它归为哪一类；
3、每一类都会有很多样本，计算这些样本的平均值作为新的中心点；
4、如果新的中心点和旧的中心点的差别不大，则完成聚类，否则重新跳至第二步；




近邻传播算法也是一种聚类算法，而网上描述的内容也非常少，包括我查阅了周志华大佬的西瓜书，也没找到对应的内容，百度百科内容如下：



2007年，Frey和Dueck在《Science》上首次提出了近邻传播算法，能够在较短的时间内处理大规模数据集，得到较理想的结果。该方法是一种基于实例的方法（Instance-based）,与经典的k-means算法具有相同的目标函数，但其在算法原理上与k-means算法存在很大的不同。近邻传播算法是一种基于近邻信息传递的聚类算法，该算法以数据集的相似度矩阵作为输入，算法起始阶段将所有的样本看作是潜在的聚类中心点，同时，将每个样本点都视为网络中的一个节点，吸引力信息沿着节点连线递归传输，直到找到最优的类代表点集合（类代表点必须是实际数据集中的点，成exemplar）,使得所有数据点到最近的类代表点的相似度之和最大。其中，吸引力信息是数据点适合被选作其他数据的类代表点的程度。



相比较于其他传统的聚类算法，AP算法将每个数据点都作为候选的类代表点，避免了聚类结果受限于初始类代表点的选择，同时该算法对于数据集生成的相似度矩阵的对称性没有要求，并在处理大规模多类数据时运算速度快，所以能够很好地解决非欧空间问题（如不满足对称性或三角不等式）以及大规模稀疏矩阵计算问题等。



相比于k-means和k-中心算法，AP算法的优化过程具有更高的鲁棒性。前两种算法都是采用的贪婪算法来解决组合优化问题，AP算法是一种连续优化的过程，每个样本点都被视为候选类代表点，聚类是逐渐被识别出来的。尤其是，AP算法不受初始点选择的困扰，而且能够保证收敛到全局最优。实际中，AP算法能获得比k-means更稳定的结果。



由于近邻传播算法只需要进行简单的局部计算，因此，与传统聚类算法相比，它能够在更短的CPU运算时间里达到更好的聚类效果。当数据集规模较小时，近邻传播算法与传统算法的差别不大，优势不明显；但是当数据集规模增大时，或者说，聚类算法的特征矩阵变得高维稀疏时，近邻传播算法性能明显优于传统算法。目前该算法已经成功应用于人脸识别，基因发现、网络文本挖掘、图像分割以及最优航线设计等领域。


接下来进入近邻传播算法（AP算法）

几个常用的名词：



exemplar：指的是聚类中心。

similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算。所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。

Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度，即吸引度。

Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度，即归属度。



AP的输入是一个节点间的相似度矩阵，其中S(i,j)表示节点i和节点j之间的相似度，S(k,k)表示节点k作为k的聚类中心的合适程度，可以理解为，节点k成为聚类中心合适度，在最开始时，这个值是初始化的时候使用者给定的值，会影响到最后聚类的数量。




算法描述：

   假设{x1,x2,⋯,xn}{x1,x2,⋯,xn}数据样本集，数据间没有内在结构的假设。令S是一个刻画点之间相似度的矩阵，使得s(i,j)>s(i,k)s(i,j)>s(i,k)当且仅当xixi与xjxj的相似性程度要大于其与xkxk的相似性。

AP算法进行交替两个消息传递的步骤，以更新两个矩阵：

吸引信息（responsibility）矩阵R：r(i,k)r(i,k)描述了数据对象k适合作为数据对象i的聚类中心的程度，表示的是从i到k的消息；
归属信息（availability）矩阵A：a(i,k)a(i,k)描述了数据对象i选择数据对象k作为其据聚类中心的适合程度，表示从k到i的消息。
 

    两个矩阵R ,A中的全部初始化为0. 可以看成Log-概率表。这个算法通过以下步骤迭代进行：
    
![简陋的草图](http://img.blog.csdn.net/20180124223605719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveXVpaDM0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


对以上步骤进行迭代，如果这些决策经过若干次迭代之后保持不变或者算法执行超过设定的迭代次数，又或者一个小区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。


公式解释




AP中节点间传递的消息为两类：吸引度和归属度。

首先，吸引度是节点i传递向节点k的信息，传达了节点k对节点i的吸引度，记为r(i,k)，那么如何来衡量这个吸引度，其实吸引度是一个相对的概念，先前我们有相似度矩阵记录了k成为i的聚类中心的合适程度，那么这里我们只需要证明k比其他节点更合适了就可以了，那么其他节点是否合适这个如何进行衡量呢，是否合适其实就是看这两个节点是否相互认可，对于其他节点k'我们有s(i,k')表示节点k'作为节点i的聚类中心的合适度，那么再定义一个a(i,k')表示i对节点k'的认可程度（归属度），这两个值相加，a(i,k') + s(i,k')，就可以计算出节点k'作为节点i的聚类中心的合适程度了，这里，在所有其他节点k'中，找出最大的a(i,k') + s(i,k')，即max{a(i,k’)+s(i,k')}，再使用s(i,k) - max{a(i,k’)+s(i,k')} 就可以得出k对i的吸引度了，也就是第一个公式：

r(i,k) = s(i,k) - max{a(i,k’)+s(i,k')} 其中k != k'

接下来计算上面提到的归属度a(i,k)，表示了节点i选择节点k作为它的聚类中心的合适程度，这里要考虑到的一个思想是：如果节点k作为其他节点i'的聚类中心的合适度很大，那么节点k作为节点i的聚类中心的合适度也可能会较大，由此就可以先计算节点k对其他节点的吸引度，r(i',k)，然后做一个累加和表示节点k对其他节点的吸引度：

∑max{0，r(i',k)} ps.这里在r(i',k)跟0之间取一个大的原因是因为s(i',k)一般会初始化成负值，导致r(i',k)计算出来也有可能是负值，这样的好处是，最后可以方便找出合适的聚类中心在完成所有计算后。

然后再加上r(k,k)，这里为什么要加上r(k,k)，根据吸引度公式，我们可以看出，其实r(k,k)，反应的是节点k有多不适合被划分到其他聚类中心下去，这里的公式中，将k有多适合成为其他节点的聚类中心：∑max{0，r(i',k)}加上它有多不适合被划分到其他聚类中心下去：r(k,k) 就有了计算公式：

a(i,k)=min{0，r(k,k)+∑max{0，r(i',k)}} 为了不让这个值过大，影响整体结果，将这个值控制在0以下。

其中a(k,k)的定义稍微有些不一样，只用∑max{0，r(i',k)}就可以了 主要反映k作为聚类中心的能力。










参考：http://blog.csdn.net/doctor_feng/article/details/18778921

参考：http://blog.csdn.net/lixi__liu/article/details/48470173

参考：http://www.chinaaet.com/article/79936
